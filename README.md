# The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs

This is the official implementation of "The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs".
[Hong Li](https://github.com/lihong2303), [Nanxi Li](https://github.com/andylinx), [Yuanjie Chen](https://github.com/ccmoony),[Jianbin Zhu](https://github.com/Peebinens), [Qinlu Guo](https://github.com/ggsdeath), [Cewu Lu](https://www.mvig.org), [Yong-Lu Li](https://dirtyharrylyl.github.io).

## Overview

![Alt Text](./Images/teaser_figure.png)

In this paper, we first devise a standard association benchmark based on adjective and verb association semantic concepts. Instead of costly data annotation and organization, we propose a convenient annotation-free reconstruction method transforming the general dataset for our association tasks. Furthermore, we comprehensively investigate the MLLMsâ€™ ability and potential for associative ability.

## Dataset

We reconstructed two association datasets based on adjective and verb concepts, for details on how to download the dataset and the structure please refer to [Data](./data/Data.md).

## Installation
For Python environment, see [requirements.txt](requirements.txt)

## Usage



## Reference
```
@misc{li2024labyrinthlinksnavigatingassociative,
      title={The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs}, 
      author={Hong Li and Nanxi Li and Yuanjie Chen and Jianbin Zhu and Qinlu Guo and Cewu Lu and Yong-Lu Li},
      year={2024},
      eprint={2410.01417},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.01417}, 
}
```

## Acknowledgement

We extend our gratitude to the prior outstanding work in object concept learning, particularly [OCL](https://github.com/silicx/ObjectConceptLearning) and [Pangea](https://github.com/DirtyHarryLYL/Sandwich), which serve as the foundation for our research.

