# The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs

This is the official implementation of "The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs".
[Hong Li](https://github.com/lihong2303), [Nanxi Li](https://github.com/andylinx), [Yuanjie Chen](https://github.com/ccmoony),[Jianbin Zhu](https://github.com/Peebinens), [Qinlu Guo](https://github.com/ggsdeath), [Cewu Lu](https://www.mvig.org), [Yong-Lu Li](https://dirtyharrylyl.github.io).

## Overview

![Alt Text](./Images/teaser_figure.png)

In this paper, we first devise a standard association benchmark based on adjective and verb association semantic concepts. Instead of costly data annotation and organization, we propose a convenient annotation-free reconstruction method transforming the general dataset for our association tasks. Furthermore, we comprehensively investigate the MLLMsâ€™ ability and potential for associative ability.

## Dataset

We reconstructed two association datasets based on adjective and verb concepts, for details on how to download the dataset and the structure please refer to [Data](./data/Data.md).

## Installation
For Python environment, see [requirements.txt](requirements.txt)

## Usage



## Reference
```
@article{li2024labyrinth,
  title={The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs},
  author={Li, Hong and Li, Nanxi and Chen, Yuanjie and Zhu, Jianbin and Guo, Qinlu and Lu, Cewu and Li, Yong-Lu},
  journal={arXiv preprint arXiv:2410.01417},
  year={2024}
}
```

## Acknowledgement

We extend our gratitude to the prior outstanding work in object concept learning, particularly [OCL](https://github.com/silicx/ObjectConceptLearning) and [Pangea](https://github.com/DirtyHarryLYL/Sandwich), which serve as the foundation for our research.

